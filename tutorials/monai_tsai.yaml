# 2D Worm Segmentation with MONAI Residual UNet
# Electron microscopy (EM) dataset - 2D slices
#
# This config demonstrates 2D segmentation using MONAI's UNet with residual units.
#
# Key features for 2D training:
# - Proper 2D architecture (spatial_dims=2, auto-inferred from input_size)
# - NEW: data_transform for paired image/label transforms (resize, crop, etc.)
# - image_transform for image-only processing (normalization, intensity)
# - Optimized batch size and data loading for 2D slices
# - Augmentation pipeline tuned for 2D EM data
#
# Transform pipeline explanation:
#   data_transform  → Applied to BOTH image and label (keeps them aligned)
#                     Example: resize, spatial augmentations
#                     Uses bilinear for images, nearest-neighbor for labels
#   image_transform → Applied to image ONLY (intensity/normalization)
#                     Example: normalize to [0,1], clip percentiles

experiment_name: monai_tsai  # Will be overridden by YAML filename
description: 3D axon segmentation using MONAI Residual UNet with paired data transforms

# System - Optimized for 2D training
system:
  training:
    num_gpus: 4                          # Single GPU
    num_cpus: 8                          # Increase for better data loading
    num_workers: 8                        # Parallel data loading (2D slices are lighter)
    batch_size: 8                        # Higher batch size for 2D (vs 4 for 3D)
  inference:
    num_gpus: 1
    num_cpus: 4
    num_workers: 4
    batch_size: 32                       # Process multiple 2D slices in parallel
  seed: 42                               # Reproducibility

# Model - MONAI UNet with residual units (2D)
model:
  architecture: monai_unet
  input_size: [64, 64, 64]               # 3D input (spatial_dims auto-inferred from input_size length)
  output_size: [64, 64, 64]
  in_channels: 1
  out_channels: 1                      

  # UNet architecture configuration
  filters: [28, 36, 48, 64]      # Standard UNet channel progression
  num_res_units: 2                     # Residual units per block
  kernel_size: 3                       # Convolution kernel size
  norm: batch                          # Batch normalization
  dropout: 0.1                         # Dropout for regularization

  # Loss configuration - Dice for overlap, BCE for pixel-wise accuracy
  loss_functions: [WeightedBCE, DiceLoss]
  loss_weights: [1.0, 1.0]             # Equal weighting for BCE and Dice
  loss_kwargs:
  - {reduction: mean}
  - {include_background: true, sigmoid: true, smooth_nr: 1e-5, smooth_dr: 1e-5}

# Data - Using automatic 80/20 train/val split (DeepEM-style)
data:  
  # Volume configuration
  train_image: datasets/axon_data_30pc_subset/training/training-original/volumes/*.tiff
  train_label: datasets/axon_data_30pc_subset/training/training-original/labels/*.tiff
  train_resolution: [5, 5]        # Lucchi EM: 5nm isotropic resolution
  use_preloaded_cache: true            # Load volumes into memory for fast training

  # Patch configuration
  patch_size: [64, 64, 64]          # Larger patches for better context
  pad_size: [0, 0, 0]                 # Padding for valid convolutions
  pad_mode: reflect                    # Reflection padding at boundaries
  iter_num_per_epoch: 1280             # 1280 random crops per epoch

  # Data transformation (applied to image/label/mask for spatial alignment)
  # NEW: Paired transforms ensure image and label stay aligned
  

  # Image normalization (applied to image only)
  image_transform:
    normalize: "0-1"                     # Min-max normalization to [0, 1]
    clip_percentile_low: 0.05             # 5th percentile intensity clipping
    clip_percentile_high: 0.95             # 95th percentile intensity clipping
  label_transform:
    targets:
      - name: binary  # Ensures labels are {0, 1}
  
  # Augmentation - Enabled for better generalization
  augmentation:
    preset: some                        # Enable data augmentation
    flip:
      enabled: true      
      spatial_axis: [0, 1, 2]  # Flip x/y/z
    rotate:
      enabled: true
      spatial_axes: [0, 1, 2]
    affine:
      enabled: true
      prob: 0.5
      rotate_range: [0.2, 0.2, 0.2]
    intensity:
      enabled: true
      shift_intensity_prob: 0.8
      shift_intensity_offset: 0.3
    

# Optimizer - AdamW with optimized hyperparameters
optimization:
  max_epochs: 1000
  gradient_clip_val: 1.0               # Higher clip (0.5 was too aggressive)
  accumulate_grad_batches: 1
  precision: "bf16-mixed"              # BFloat16 mixed precision
  
  optimizer:
    name: AdamW
    lr: 0.001                            # Higher initial LR (will use warmup)
    weight_decay: 0.01                   # L2 regularization
    betas: [0.9, 0.999]                  # Standard Adam betas (momentum terms)
    eps: 1.0e-8                          # Numerical stability

  # Scheduler - Cosine annealing with warmup for smooth convergence
  scheduler:
    name: ReduceLROnPlateau           # Reduce LR when validation loss plateaus
    mode: min                         # Monitor minimum loss
    factor: 0.5                       # Reduce LR by 50%
    patience: 50                      # Wait 50 epochs before reducing
    threshold: 1.0e-4                 # Minimum change to qualify as improvement
    min_lr: 1.0e-6                    # Don't go below 1e-6

monitor:
  # Loss monitoring and validation frequency  
  detect_anomaly: false
  logging:
    # scalar loss
    scalar:
      loss: [train_loss_total_epoch]
      loss_every_n_steps: 10
      val_check_interval: 1.0
      benchmark: true
    
    # visualization
    images:
      enabled: true
      max_images: 8
      num_slices: 2
      log_every_n_epochs: 1                # Log every N epochs (default: 1)
      channel_mode: argmax                 # 'argmax', 'all', or 'selected'
      selected_channels: null              # Only used when channel_mode='selected'
  
  # Checkpointing
  checkpoint:
    mode: min
    save_top_k: 1
    save_last: true
    save_every_n_epochs: 10
    dirpath: outputs/monai_tsai/checkpoints/  # Will be dynamically set to outputs/{yaml_filename}/YYYYMMDD_HHMMSS/checkpoints/
    # checkpoint_filename: auto-generated from monitor metric (epoch={epoch:03d}-{monitor}={value:.4f})
    use_timestamp: true       # Enable timestamped subdirectories (YYYYMMDD_HHMMSS)

  # Early stopping - More patient for better convergence
  early_stopping: 
    enabled: true
    monitor: train_loss_total_epoch
    patience: 300         # Increased patience (was 200)
    mode: min
    min_delta: 1.0e-5    # Smaller threshold for finer convergence
    check_finite: true   # Stop if monitored metric becomes NaN/inf
    threshold: 0.01      # Stop if loss gets this low (excellent convergence)
    divergence_threshold: 2.0  # Stop if loss exceeds this (training collapse)

# Inference - MONAI SlidingWindowInferer
inference:
  data:
    test_image: datasets/axon_data_30pc_subset/validation/validation-original/volumes/*.tiff
    test_label: datasets/axon_data_30pc_subset/validation/validation-original/labels/*.tiff
    test_resolution: [5, 5]
    output_path: outputs/monai_tsai/results/

  # MONAI SlidingWindowInferer parameters
  sliding_window:
    window_size: null               # Disable sliding window for 2D data (use direct inference)
    # sw_batch_size: automatically set from system.inference.batch_size (currently 32)
    overlap: 0.5                         # 50% overlap between patches
    blending: gaussian                   # Gaussian weighting for smooth blending
    sigma_scale: 0.25                   # Gaussian sigma scale (larger = smoother blending, default: 0.125)
    padding_mode: reflect                # Reflection-padding at volume boundaries

  # Test-Time Augmentation (TTA)
  test_time_augmentation:
    enabled: true        # Enable TTA for improved predictions#
    flip_axes: all                   # Flip strategy: "all" (8 flips, 8x slower), null (no aug, 1x), or custom list
    #tta_flip_axes: [[1]]                   # Flip Z-axis only (MUST use [1] for depth in BCDHW, NOT [0]!)
    # Example custom TTA: [[1], [2], [3]] for single-axis flips only (Z, Y, X) - 4x inference
    # Example custom TTA: [[1, 2], [1, 3], [2, 3]] for two-axis flips only - 4x inference
    channel_activations: [[0, 1, 'sigmoid']]  # Per-channel activations: apply softmax to all channels (0-1)
    ensemble_mode: mean              # Ensemble strategy: 'mean' (average), 'min' (conservative), 'max' (aggressive)
    # NOTE: tta_act and tta_channel are applied even with null flip_axes (no ensemble, just activation + channel selection)
    # NOTE: If tta_channel selects specific channels, loss computation will be skipped (loss needs all class channels)


# Decoding: predicted feature maps to segmetnation mask (semantic or instance segmentation)
  decoding:
    - name: binary_thresholding
      kwargs:
        threshold_range: [0.5, 1]          # binary thresholding range
        opening_iterations: 2              # Morphological opening (erosion + dilation) to remove small noise
        connected_components:
          enabled: true                    # Enable connected components filtering
          remove_small: 10                 # Remove small objects with size less than 10 pixels
          connectivity: 26                 # Face connectivity (4=4-connected for 2D, 6=6-connected for 3D)


  # Postprocessing configuration (applied AFTER decoding)
  postprocessing:

    # Output format (intensity scaling and dtype conversion)
    intensity_scale: 255                 # Scale predictions to [0, 255] for saving
    intensity_dtype: uint8               # Save as uint8


  # Evaluation
  evaluation:
    enabled: true                        # Use eval mode for BatchNorm
    metrics: [adapted_rand]              # Metrics to compute (adapted_rand for instance segmentation)

  # NOTE: batch_size=1 for inference
  #   During training: batch_size controls how many random patches to load
  #   During inference: batch_size=1 means process one full volume at a time
  #   sw_batch_size (above) controls how many patches are processed per GPU forward pass
