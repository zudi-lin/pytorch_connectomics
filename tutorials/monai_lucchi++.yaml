# Lucchi mitochondria segmentation with RSUNet
# Electron microscopy (EM) dataset
#
# This config uses RSUNet - a residual symmetric U-Net specifically designed
# for EM image segmentation. RSUNet avoids checkerboard artifacts through:
# - No transposed convolutions (uses upsample + conv instead)
# - Anisotropic convolutions optimized for EM data
# - Proven performance on connectomics benchmarks

experiment_name: lucchi++_rsunet
description: Mitochondria segmentation on Lucchi++ EM dataset using RSUNet (no checkerboard artifacts)

# System
system:
  training:
    num_gpus: 4
    num_cpus: 8
    num_workers: 8           # Set to 0 to avoid /dev/shm space issues (use in-process loading)
    batch_size: 4
  inference:
    num_gpus: 1
    num_cpus: 1
    num_workers: 1           # Set to 0 to avoid /dev/shm space issues
    batch_size: 16
  seed: 42

# Model - RSUNet (Residual Symmetric U-Net)
model:
  architecture: monai_basic_unet3d
  input_size: [112, 112, 112]
  output_size: [112, 112, 112]
  in_channels: 1
  out_channels: 1                      # Single channel with BCE loss (standard for EM)

  filters: [32, 64, 128, 256]          # 4-level encoder (standard for RSUNet)
  dropout: 0.1
  activation: leakyrelu
  norm: batch
  upsample: nontrainable  # upsampling + convolution instead of transposed convolution
  
  # Loss configuration - WeightedBCE + Dice for mitochondria segmentation
  loss_functions: [WeightedBCE, DiceLoss]
  loss_weights: [1.0, 1.0]             # Equal weighting for BCE and Dice
  loss_kwargs:
    - {reduction: mean}                            # WeightedBCE: average over batch (reduction='mean' replaces deprecated size_average)
    - {include_background: false, sigmoid: true, smooth_nr: 1e-5, smooth_dr: 1e-5}  # DiceLoss with sigmoid activation

# Data - Using automatic 80/20 train/val split (DeepEM-style)
data:
  # Volume configuration
  train_image: datasets/Lucchi++/train_im.h5
  train_label: datasets/Lucchi++/train_mito.h5
  train_resolution: [5, 5, 5]        # Lucchi EM: 5nm isotropic resolution
  use_preloaded_cache: true            # Load volumes into memory for fast training

  # Patch configuration
  patch_size: [112, 112, 112]           # Isotropic patches for training
  pad_size: [0, 0, 0]                   # No padding during training (not needed)
  iter_num_per_epoch: 1280              # 1280 random crops per epoch
  
  # Image normalization
  image_transform:    
    normalize: "0-1"                   # Min-max normalization to [0, 1]
    clip_percentile_low: 0.0           # No clipping
    clip_percentile_high: 1.0
  
  # Augmentation - moderate set for 3D mitochondria segmentation
  # Recommended for Lucchi++: geometric transforms + EM-specific augmentations
  augmentation:
    preset: "some"  # Enable only augmentations explicitly set to enabled=True
    
    # Standard geometric augmentations (safe for 3D EM)
    flip:
      enabled: true
      prob: 0.5
      spatial_axis: [0, 1, 2]  # Flip x/y/z
    
    rotate:
      enabled: true
      prob: 0.5
      spatial_axes: [0, 1, 2]  # Rotate x/y/z
    
    affine:
      enabled: true
      prob: 0.3  # Lower prob to avoid too aggressive transforms
      rotate_range: [0.1, 0.1, 0.1]  # Small rotations (~6°) - careful with Z-axis
      scale_range: [0.05, 0.05, 0.05]  # Small scaling (±5%)
      shear_range: [0.05, 0.05, 0.05]  # Small shearing
    
    # Intensity augmentations (important for EM data variability)
    intensity:
      enabled: true
      gaussian_noise_prob: 0.2  # Moderate noise
      gaussian_noise_std: 0.03
      shift_intensity_prob: 0.4
      shift_intensity_offset: 0.1
      contrast_prob: 0.4
      contrast_range: [0.8, 1.2]  # Moderate contrast variation
    
    # EM-specific augmentations (highly recommended for EM data)
    misalignment:
      enabled: true
      prob: 0.4
      displacement: 8  # Moderate displacement for small patches
      rotate_ratio: 0.3  # Mix of translation and rotation
    
    missing_section:
      enabled: true
      prob: 0.3
      num_sections: 2  # 1-2 missing sections (common in EM)
    
    motion_blur:
      enabled: true
      prob: 0.3
      sections: 2
      kernel_size: 9  # Moderate blur
    
    # Avoid these for mitochondria segmentation:
    # - elastic: Too aggressive for small structures
    # - cut_blur/cut_noise: May be too aggressive
    # - copy_paste/mixup: Not needed for binary segmentation 


# Optimizer - Adam with conservative hyperparameters (proven for EM segmentation)
optimization:
  max_epochs: 1000                      # Reduced epochs (RSUNet converges faster)
  gradient_clip_val: 0.5               # Conservative gradient clipping
  accumulate_grad_batches: 1
  precision: "bf16-mixed"              # BFloat16 mixed precision

  optimizer:
    name: Adam                         # Standard Adam (not AdamW for EM tasks)
    lr: 0.0001                         # Conservative LR (1e-4 is standard for EM)
    weight_decay: 0.0                  # No weight decay (not beneficial for EM)
    betas: [0.9, 0.999]                # Standard Adam betas
    eps: 1.0e-8                        # Numerical stability

  # Scheduler - ReduceLROnPlateau for adaptive learning
  scheduler:
    name: ReduceLROnPlateau           # Reduce LR when validation loss plateaus
    mode: min                         # Monitor minimum loss
    factor: 0.5                       # Reduce LR by 50%
    patience: 50                      # Wait 50 epochs before reducing
    threshold: 1.0e-4                 # Minimum change to qualify as improvement
    min_lr: 1.0e-6                    # Don't go below 1e-6

monitor:
  # Loss monitoring and validation frequency  
  detect_anomaly: false
  logging:
    # scalar loss
    scalar:
      loss: [train_loss_total_epoch]
      loss_every_n_steps: 10
      val_check_interval: 1.0
      benchmark: true
    
    # visualization
    images:
      enabled: true
      max_images: 8
      num_slices: 2
      log_every_n_epochs: 1                # Log every N epochs (default: 1)
      channel_mode: argmax                 # 'argmax', 'all', or 'selected'
      selected_channels: null              # Only used when channel_mode='selected'
  
  # Checkpointing
  checkpoint:
    mode: min
    save_top_k: 1
    save_last: true
    save_every_n_epochs: 10
    # checkpoint_filename: auto-generated from monitor metric (epoch={epoch:03d}-{monitor}={value:.4f})
    use_timestamp: true       # Enable timestamped subdirectories (YYYYMMDD_HHMMSS)

  # Early stopping - Patient for convergence
  early_stopping:
    enabled: true
    monitor: train_loss_total_epoch
    patience: 100        # Patient waiting for improvement
    mode: min
    min_delta: 1.0e-4    # Minimum delta for improvement
    check_finite: true   # Stop if monitored metric becomes NaN/inf
    threshold: 0.02      # Stop if loss gets this low (excellent convergence for EM)
    divergence_threshold: 2.0  # Stop if loss exceeds this (training collapse)

# Inference - MONAI SlidingWindowInferer
inference:
  data:
    #test_image: datasets/Lucchi++/test_im.h5
    #test_label: datasets/Lucchi++/test_mito.h5
    test_image: datasets/Lucchi++/train_im.h5
    test_label: datasets/Lucchi++/train_mito.h5
    test_resolution: [5, 5, 5]

  # MONAI SlidingWindowInferer parameters
  sliding_window:
    window_size: [112, 112, 112]          # Patch size (matches training patches)
    # sw_batch_size: automatically set from system.inference.batch_size (currently 32)
    overlap: 0.5                         # 75% overlap at boundaries (fixes z=0 artifacts)
    blending: gaussian                    # Gaussian weighting for smooth blending
    sigma_scale: 0.25                     # Larger sigma = smoother blending at boundaries
    padding_mode: replicate               # Replicate edge values (better than reflect for z=0)

  # Test-Time Augmentation (TTA)
  test_time_augmentation:
    enabled: true        # Enable TTA for improved predictions
    flip_axes: null      # No flip augmentation (set to null to disable flips)
    # XY flips are safe for isotropic XY resolution, Z-flip avoided due to anisotropy
    channel_activations: [[0, 1, 'sigmoid']]  # Sigmoid activation for single-channel output
    select_channel: null                   # Keep all channels (single channel output)
    ensemble_mode: mean              # Mean ensemble (smooth predictions)
    # NOTE: Reduced TTA compared to original (4x instead of 8x) for faster inference


  # Postprocessing configuration (applied AFTER TTA if enabled)
  postprocessing:
    intensity_scale: 255                    # Scale predictions to [0, 255] for saving
    intensity_dtype: uint8                  # Save as uint8

  # Evaluation
  evaluation:
    enabled: true                        # Use eval mode for BatchNorm
    metrics: [jaccard]             # Metrics to compute

  # NOTE: batch_size=1 for inference
  #   During training: batch_size controls how many random patches to load
  #   During inference: batch_size=1 means process one full volume at a time
  #   sw_batch_size (above) controls how many patches are processed per GPU forward pass
