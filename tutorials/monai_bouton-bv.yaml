# Hydra big vesicle segmentation with MONAI Residual UNet
# Multi-task learning: Binary + Boundary + Distance
#
# This config uses MONAI's UNet with residual units for big vesicle segmentation
# with multi-task learning to predict:
#   - Channel 0: Binary masks (sigmoid activation)
#   - Channel 1: Boundary maps (sigmoid activation) 
#   - Channel 2: Distance transforms (tanh activation)
#
# Multi-task setup uses different loss functions for each channel:
#   - Binary & Boundary: DiceLoss + BCEWithLogitsLoss
#   - Distance: WeightedMSELoss
#
# Valid region masks (train_mask, val_mask):
#   - Masks define valid regions for loss computation
#   - Applied after same augmentation as labels (spatial consistency)
#   - Loss is only computed in valid (mask=1) regions

experiment_name: hydra_bv_monai_unet
description: Hydra big vesicle segmentation with MONAI Residual UNet and multi-task learning

# System
system:
  training:
    num_gpus: 4
    num_cpus: 8
    num_workers: 8                     # More workers for parallel data loading (6x speedup)
    batch_size: 32                      # Larger batch for smaller patches
  inference:
    num_gpus: 1
    num_cpus: 1
    num_workers: 1
    batch_size: 1
  seed: 42

# Model - MONAI UNet with residual units for multi-task learning
model:
  architecture: monai_unet
  input_size: [24, 96, 96]             # 64x64x64 input patches
  output_size: [24, 96, 96]            # 64x64x64 output patches
  in_channels: 1
  out_channels: 3                      # 3 channels: binary, contour, distance

  # UNet architecture configuration (optimized for 64^3)
  filters: [32, 64, 128, 256]          # 4 levels for 64^3 (64->32->16->8->4)
  strides: [2, 2, 2]                   # 3 downsampling levels
  num_res_units: 2                     # Residual units per block
  kernel_size: 3                       # Convolution kernel size
  norm: batch
  dropout: 0.0                         # No dropout for nucleus segmentation

  # Multi-task loss configuration
  loss_functions: [DiceLoss, BCEWithLogitsLoss, WeightedMSE]
  loss_weights: [1.0, 0.5, 2.0]       # Binary: Dice+BCE, Contour: Dice+BCE, Distance: MSE
  loss_kwargs:
    - {sigmoid: true, smooth_nr: 1e-5, smooth_dr: 1e-5}  # DiceLoss for binary
    - {}                                 # BCEWithLogitsLoss for binary
    - {tanh: true}                       # WeightedMSE for distance (with tanh activation)

  # Multi-task configuration
  # Format: [[start_ch, end_ch, target_name, loss_indices], ...]
  multi_task_config:
    - [0, 1, "label", [0, 1]]          # Original labels: Dice + BCE
    - [1, 2, "boundary", [0, 1]]       # Boundary channel: Dice + BCE  
    - [2, 3, "edt", [2]]               # Distance channel: MSE

# Data - Hydra big vesicle dataset (multiple volumes)
data:
  # Volume configuration - supports multiple files via glob pattern or list
  train_image: "datasets/hydra-bv/vol*_im.h5"     # 27 training volumes
  train_label: "datasets/hydra-bv/vol*_vesicle_ins.h5"     # 27 training labels
  train_mask: "datasets/hydra-bv/vol*_mask.h5"     # 27 training labels
  train_resolution: [30, 8, 8]   # NucMM: 1.0 isotropic resolution
  use_preloaded_cache: true           # Pre-load all volumes into memory (MAJOR speedup)
  cache_rate: 1.0                     # Cache all volumes in RAM
  persistent_workers: true            # Keep workers alive between epochs (avoid restart overhead)

  # Patch configuration
  patch_size: [24, 96, 96]             # 64x64x64 training patches
  pad_size: [4, 16, 16]               # Reflection padding for context
  pad_mode: reflect                    # Reflection padding at boundaries
  iter_num_per_epoch: 1280              # More iterations for smaller patches
  
  # Image normalization
  image_transform:
    normalize: "0-1"                   # Min-max normalization to [0, 1]
    clip_percentile_low: 0.0           # No clipping
    clip_percentile_high: 1.0

  # Label transformation for multi-task learning
  label_transform:
    targets:
      - name: binary                # Channel 0: foreground mask
      - name: instance_boundary     # Channel 1: contour map
        kwargs:
          thickness: 1
          edge_mode: "all"
          mode: "2d"
      - name: instance_edt          # Channel 2: distance transform (bbox-optimized)
        kwargs:
          mode: "3d"              # 2D EDT with per-instance bounding box optimization
          quantize: false

  # Augmentation
  # Note: When enabled, augmentations (rotation, flip, etc.) are applied to:
  #   - image: geometric + intensity transforms
  #   - label: same geometric transforms as image
  #   - mask: same geometric transforms as image and label
  # This ensures spatial consistency across image/label/mask triplets
  augmentation:
    enabled: true

# Optimizer - AdamW with NucMM-specific hyperparameters
optimization:
  max_epochs: 10000
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: "bf16-mixed"              # BFloat16 mixed precision
  
  optimizer:
    name: AdamW
    lr: 0.001                          # Lower LR for smaller patches (64^3)
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1.0e-8

  # Scheduler - Cosine annealing with warmup
  scheduler:
    name: CosineAnnealingLR
    warmup_epochs: 5                   # Shorter warmup for 64^3
    warmup_start_lr: 1.0e-5
    min_lr: 1.0e-6
    t_max: 9995

monitor:
  # Loss monitoring and validation frequency  
  detect_anomaly: false
  logging:
    # scalar loss
    scalar:
      loss: [train_loss_total_epoch]
      loss_every_n_steps: 10
      val_check_interval: 1.0
      benchmark: true
    
    # visualization
    images:
      enabled: true
      max_images: 2
      num_slices: 4
      log_every_n_epochs: 1
      channel_mode: all                 # Show all 3 channels for multi-task
      selected_channels: null
  
  # Checkpointing
  checkpoint:
    mode: min
    save_top_k: 3
    save_last: true
    save_every_n_epochs: 10
    dirpath: outputs/hydra_bv_monai_unet/checkpoints/
    use_timestamp: true

  # Early stopping
  early_stopping: 
    enabled: true
    monitor: train_loss_total_epoch
    patience: 300
    mode: min
    min_delta: 1.0e-5
    check_finite: true
    threshold: 0.01
    divergence_threshold: 100.0

# Inference - MONAI SlidingWindowInferer for NucMM
inference:
  data:
    test_image: datasets/bouton-bv/Bouton_000.tiff    
    test_transpose: [2,1,0] # xyz to zyx
    test_resolution: [30, 8, 8]
    output_path: outputs/bouton_bv_monai_unet/results/

  # MONAI SlidingWindowInferer parameters
  sliding_window:
    window_size: [24, 96, 96]          # Match training patch size    
    overlap: 0.5                       # 50% overlap between patches
    blending: gaussian                 # Gaussian weighting for smooth blending
    sigma_scale: 0.25
    padding_mode: reflect              # Reflection-padding at volume boundaries

  # Test-Time Augmentation (TTA)
  test_time_augmentation:
    enabled: true
    flip_axes: null                     # Use all 8 flip augmentations
    # Per-channel activations (aligned with multi_task_config)
    # Format: [[channel_idx, activation], ...]
    channel_activations:
      - [0, sigmoid]                   # Channel 0: binary segmentation (sigmoid)
      - [1, sigmoid]                   # Channel 1: boundary (sigmoid)
      - [2, tanh]                      # Channel 2: EDT distance (no activation)
    select_channel: all               # Use all channels
    ensemble_mode: mean
    apply_mask: false                   # Multiply predictions by test_mask after ensemble
    save_predictions: true             # Save intermediate predictions (before decoding)

  # Decoding configuration (instance segmentation postprocessing)
  decoding:
    - name: decode_binary_contour_distance_watershed
      kwargs:
        binary_threshold: [0.9, 0.85]
        contour_threshold: [0.8, 1.1]
        distance_threshold: [0.5, 0]
        min_instance_size: 16
        min_seed_size: 8
        prediction_scale: 1    

  postprocessing:
    output_transpose: [2,1,0]

  # Evaluation
  evaluation:
    enabled: false
    metrics: [adapted_rand]
